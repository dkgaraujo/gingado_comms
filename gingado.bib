@article{ModelCards,
  author    = {Margaret Mitchell and
               Simone Wu and
               Andrew Zaldivar and
               Parker Barnes and
               Lucy Vasserman and
               Ben Hutchinson and
               Elena Spitzer and
               Inioluwa Deborah Raji and
               Timnit Gebru},
  title     = {Model Cards for Model Reporting},
  journal   = {CoRR},
  volume    = {abs/1810.03993},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.03993},
  eprinttype = {arXiv},
  eprint    = {1810.03993},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-03993.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{fastai,
AUTHOR = {Howard, Jeremy and Gugger, Sylvain},
TITLE = {Fastai: A Layered API for Deep Learning},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {108},
URL = {https://www.mdpi.com/2078-2489/11/2/108},
ISSN = {2078-2489},
ABSTRACT = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4&ndash;5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
DOI = {10.3390/info11020108}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@misc{syft,
      title={A generic framework for privacy preserving deep learning}, 
      author={Theo Ryffel and Andrew Trask and Morten Dahl and Bobby Wagner and Jason Mancuso and Daniel Rueckert and Jonathan Passerat-Palmbach},
      year={2018},
      eprint={1811.04017},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{predunequal,
author = {Fuster, Andreas and Goldsmith-Pinkham, Paul and Ramadorai, Tarun and Walther, Ansgar},
title = {Predictably Unequal? The Effects of Machine Learning on Credit Markets},
journal = {The Journal of Finance},
volume = {77},
number = {1},
pages = {5-47},
doi = {https://doi.org/10.1111/jofi.13090},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13090},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jofi.13090},
abstract = {ABSTRACT Innovations in statistical technology in functions including credit-screening have raised concerns about distributional impacts across categories such as race. Theoretically, distributional effects of better statistical technology can come from greater flexibility to uncover structural relationships or from triangulation of otherwise excluded characteristics. Using data on U.S. mortgages, we predict default using traditional and machine learning models. We find that Black and Hispanic borrowers are disproportionately less likely to gain from the introduction of machine learning. In a simple equilibrium credit market model, machine learning increases disparity in rates between and within groups, with these changes attributable primarily to greater flexibility.},
year = {2022}
}

@article{TidyData,
 title={Tidy Data},
 volume={59},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v059i10},
 doi={10.18637/jss.v059.i10},
 abstract={A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
 number={10},
 journal={Journal of Statistical Software},
 author={Wickham, Hadley},
 year={2014},
 pages={1–23}
}

@book{DeepLearning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={http://www.deeplearningbook.org},
    year={2016}
}

@Manual{reticulate,
  title = {reticulate: Interface to 'Python'},
  author = {Kevin Ushey and JJ Allaire and Yuan Tang},
  year = {2022},
  note = {https://rstudio.github.io/reticulate/,
https://github.com/rstudio/reticulate},
}

@incollection{PyTorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}


@inproceedings{xgboost, author = {Chen, Tianqi and Guestrin, Carlos}, title = {XGBoost: A Scalable Tree Boosting System}, year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2939785}, doi = {10.1145/2939672.2939785}, abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.}, booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pages = {785–794}, numpages = {10}, keywords = {large-scale machine learning}, location = {San Francisco, California, USA}, series = {KDD '16} }


@misc{TabularDeepLearning,
      title={Revisiting Deep Learning Models for Tabular Data}, 
      author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
      year={2021},
      eprint={2106.11959},
      archivePrefix={arXiv},
      url={https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html},
      primaryClass={cs.LG}
}

@misc{yi2022stock2vec,
      title={Stock2Vec: An Embedding to Improve Predictive Models for Companies}, 
      author={Ziruo Yi and Ting Xiao and Kaz-Onyeakazi Ijeoma and Ratnam Cheran and Yuvraj Baweja and Phillip Nelson},
      year={2022},
      eprint={2201.11290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{problemmetrics,
      title={The Problem with Metrics is a Fundamental Problem for AI}, 
      author={Rachel Thomas and David Uminsky},
      year={2020},
      eprint={2002.08512},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{BostonHousingEthicsProblem,
  title = {racist data destruction?},
  author={M Carlisle},
  howpublished = {\url{https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8}},
  note = {Last accessed: 2020-02-08}
}

@article{Boston,
author = {Harrison, David and Rubinfeld, Daniel},
year = {1978},
month = {03},
pages = {81-102},
title = {Hedonic housing prices and the demand for clean air},
volume = {5},
journal = {Journal of Environmental Economics and Management},
doi = {10.1016/0095-0696(78)90006-2}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{onnx,
    author = {Bai, Junjie and Lu, Fang and Zhang, Ke and others},
    title = {ONNX: Open Neural Network Exchange},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/onnx/onnx}},
    commit = {94d238d96e3fb3a7ba34f03c284b9ad3516163be}
}

@article{fernandez2021estimating,
  title={Estimating DSGE models: Recent advances and future challenges},
  author={Fern{\'a}ndez-Villaverde, Jes{\'u}s and Guerr{\'o}n-Quintana, Pablo A},
  journal={Annual Review of Economics},
  volume={13},
  pages={229--252},
  year={2021},
  publisher={Annual Reviews}
}